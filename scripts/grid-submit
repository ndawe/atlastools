#!/usr/bin/env python

from argparse import ArgumentParser
import multiprocessing

parser = ArgumentParser(usage="%(prog)s [args] samplename1 samplename2 ...")
parser.add_argument('--nproc', type=int,
                  default=multiprocessing.cpu_count(), help="maximum number of simultaneous job submissions")
parser.add_argument('-m', '--meta', dest='metadata',
                    help='YAML file containing dataset metadata',
                    default='datasets.yml')
parser.add_argument('-v', '--version',
                    type=int,
                    help='output dataset version number',
                    default=1)
parser.add_argument('-s', '--student',
                    help='the file (excluding .py extension) containing a '
                         'class of the same name inheriting '
                         'from rootpy.batch.Student', required=True)
parser.add_argument('-u', '--user',
                    help='your grid dataset username i.e. JohnDoe',
                    required=True)
parser.add_argument('--nosite', action='store_true',
                    help='override site in metadata and let the grid decide')
parser.add_argument('-g', '--get', action='store_true',
                  help='download datasets')
parser.add_argument('-d', '--dest', default='.',
                    help='directory in which to download the datasets')
parser.add_argument('--merge', action='store_true',
                  help='merge outputs on the grid')
parser.add_argument('-f', '--filter', default=None,
                    help='only submit jobs on datasets matching this glob')
parser.add_argument('datasets', nargs="+")
args, user_args = parser.parse_known_args()

import yaml
from configobj import ConfigObj, flatten_errors
from validate import Validator
import sys
import fnmatch
import os
from glob import glob
import subprocess
import shlex
import tarfile
import uuid
import time
from cStringIO import StringIO
from contextlib import closing
import commands
import atexit


if args.metadata.endswith('.yml'):
    with open(args.metadata, 'r') as configfile:
        metadata = yaml.load(configfile)
else:
    configspec = os.path.splitext(args.metadata)[0]+'.spec'
    if not os.path.isfile(configspec):
        sys.exit('%s does not exist' % configspec)
    metadata = ConfigObj(args.metadata, configspec=configspec)
    validator = Validator()
    result = metadata.validate(validator, preserve_errors=True)
    if result != True:
        for entry in flatten_errors(metadata, result):
            # each entry is a tuple
            section_list, key, error = entry
            if key is not None:
                section_list.append(key)
            else:
                section_list.append('[missing section]')
            section_string = ', '.join(section_list)
            if error == False:
                error = 'Missing value or section.'
            print section_string, ' = ', error
        sys.exit(1)

if not args.get:
    
    # check for panda client
    status, result = commands.getstatusoutput('prun')
    if 'command not found' in result:
        sys.exit('pandaclient is not setup and prun is not in PATH. '
                 'Please setup pandaclient first.')
    import pkg_resources
    import shutil
    import stat
    filename = 'grid-setup.sh' 
    print "Copying %s from atlastools..." % filename
    setup_script = pkg_resources.resource_filename('atlastools', 'etc/%s' % filename)
    shutil.copyfile(setup_script, filename)
    os.chmod(filename, stat.S_IRUSR | stat.S_IWUSR | stat.S_IXUSR |\
                       stat.S_IRGRP | stat.S_IROTH)
    subprocess.call('./%s local' % filename, shell=True)

sorted_datasets = sorted(metadata.keys())

# expand globs
datasets = []
for dataset in args.datasets:
    if '*' in dataset:
        datasets += fnmatch.filter(sorted_datasets, dataset)
    else:
        datasets.append(dataset)

print "tarring up current directory..."
print "ignoring files/directories matching pattern in panda.ignore"
ignore_files = [] 
if os.path.isfile('panda.ignore'):
    with open('panda.ignore') as f:
        ignore_files = [os.path.normpath(line.strip()) for line in f.readlines()]

print "overriding any ignored files/directories if matching pattern in panda.include" 
include_files = []
if os.path.isfile('panda.include'):
    with open('panda.include') as f:
        include_files = [os.path.normpath(line.strip()) for line in f.readlines()]

# determine which files to include in tar
tar_files = []
for dirpath, dirnames, filenames in os.walk('.'):
    ignore_dirs = []
    for dirname in dirnames:
        fullpath = os.path.normpath(os.path.join(dirpath, dirname))
        include = True
        for pattern in ignore_files:
            if fnmatch.fnmatch(fullpath, pattern):
                include = False
                break
        # include_files will override ignore_files
        if not include:
            for pattern in include_files:
                if fnmatch.fnmatch(fullpath, pattern):
                    include = True
                    break
        if not include:
            print "ignoring all files under %s..." % fullpath
            ignore_dirs.append(dirname)
    # don't traverse ignored directories
    for ignore_dir in ignore_dirs:
        dirnames.remove(ignore_dir)
    for filename in filenames:
        include = True
        fullpath = os.path.normpath(os.path.join(dirpath, filename))
        for pattern in ignore_files:
            if fnmatch.fnmatch(fullpath, pattern):
                include = False
                break
        # include_files will override ignore_files
        if not include:
            for pattern in include_files:
                if fnmatch.fnmatch(fullpath, pattern):
                    include = True
                    break
        if include:
            print "including %s..." % fullpath
            tar_files.append(fullpath)
        else:
            print "ignoring %s..." % fullpath

# create tar file of PWD
tar_name = '%s.tar.gz' % uuid.uuid4().hex
with closing(tarfile.open(tar_name, 'w:gz')) as tar:
    for name in tar_files:
        tar.add(name)

@atexit.register
def cleanup():

    if not args.get:
        subprocess.call('./grid-setup.sh clean', shell=True)
    os.unlink(tar_name)

# remove possible .py extension
args.student = os.path.splitext(args.student)[0]

commands = []
for dataset in datasets:
    if not dataset in metadata:
        sys.exit("dataset %s not defined in metadata %s" % (dataset, args.metadata))
    inDS = metadata[dataset]['container']
    if type(inDS) not in (list, tuple):
        if os.path.isfile(inDS):
            with open(inDS) as f:
                inDS = [s.strip() for s in f.readlines()]
                inDS = [s for s in inDS if not s.startswith('#')]
    if type(inDS) not in (list, tuple):
        inDS = [inDS]
    if args.filter is not None:
        inDS = fnmatch.filter(inDS, args.filter)
    for panda_inDS in inDS:
        ds_name = panda_inDS.strip('/').replace('*','_')
        panda_outDS = 'user.%s.%s.%s.v%i' % (args.user, args.student, ds_name, args.version)
        if len(panda_outDS) > 131:
            panda_outDS = panda_outDS.replace('merge.NTUP_TAUMEDIUM.','')
        if args.get:
            if not os.path.isdir(args.dest):
                sys.exit("destination path %s does not exist" % args.dest)
            commands.append('cd %s; run -e grid dq2-get -T 3,8 %s/; cd -' % (args.dest, panda_outDS))
        else:
            panda_site = None
            if ':' in panda_inDS:
                panda_inDS, panda_site = panda_inDS.split(':') 
            panda_bexec = './grid-setup.sh build-packages'
            # `which python` is needed since setuptools rewrites the shebang
            # the sheband specified in the buildjob won't be the same as in the worker job
            panda_exec = 'source grid-setup.sh worker; ' \
                         'python ./user-python/bin/grid-batch --dataset %s ' \
                         '--metadata %s --student %s %%IN' % \
                         (dataset, args.metadata, args.student)
            panda_outputs = '%s.%s.root,cutflow.p' % (args.student, dataset)
            command = [
                'prun',
                '--bexec "%s"' % panda_bexec,
                '--exec "%s"' % panda_exec,
                '--inDS %s' % panda_inDS,
                '--outDS %s' % panda_outDS,
                '--outputs %s' % panda_outputs,
                '--inTarBall %s' % tar_name
            ]
            if args.merge:
                command += ['--mergeOutput',
                            '--mergeScript "source grid-setup.sh worker; python \`which grid-merge\` -o %OUT -i %IN"']
            if panda_site is not None and '--site' not in user_args and not args.nosite:
                command.append('--site %s' % panda_site)
            command = ' '.join(command)
            if user_args:
                command += ' %s' % ' '.join(user_args)
            commands.append(command)
        
def run(command, queue):

    output = StringIO()
    print >> output, "executing:"
    print >> output, command
    args = shlex.split(command)
    p = subprocess.Popen(args, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)
    stdout, stderr = p.communicate()
    print >> output, stdout
    queue.put(output.getvalue())

if args.get:
    print "Will get %i output datasets..." % len(commands)
else:
    print "Will submit %i jobs..." % len(commands)

processes = []
output_queue = multiprocessing.Queue()
while True:
    active = multiprocessing.active_children()
    while len(active) < args.nproc and len(commands) > 0:
        p = multiprocessing.Process(args=(commands.pop(), output_queue), target=run)
        p.start()
        processes.append(p)
        active = multiprocessing.active_children()
    while not output_queue.empty():
        print time.strftime("%a, %d %b %Y %H:%M:%S", time.localtime())
        print "%i jobs submitting and %i queued" % (len(active), len(commands))
        print output_queue.get()
    if len(commands) == 0 and len(active) == 0:
        break
    time.sleep(1)
